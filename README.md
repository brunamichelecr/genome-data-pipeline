## Data Engineering Pipeline para An√°lise Gen√©tica

## Sobre o Projeto

Este projeto demonstra a constru√ß√£o de um **pipeline completo de engenharia de dados** aplicado √† √°rea da **gen√¥mica e sa√∫de personalizada**.

A ideia √© permitir que um usu√°rio fa√ßa upload de um arquivo com seus **SNPs (rsIDs)** e, a partir da√≠:

1. Consultar APIs p√∫blicas do **NCBI (e-utilities)** para identificar **genes associados a doen√ßas**.
2. Buscar **varia√ß√µes gen√©ticas** relacionadas no **ClinVar**.
3. Obter detalhes de **SNPs no dbSNP**, incluindo riscos associados.
4. Estruturar todas as informa√ß√µes em um **banco de dados relacional (PostgreSQL)**.
5. Permitir ao usu√°rio visualizar seus resultados via um **frontend simples (Bootstrap)**.

 O objetivo principal √© **demonstrar compet√™ncias s√≥lidas em engenharia de dados**:

* Integra√ß√£o de m√∫ltiplas fontes externas.
* Cria√ß√£o de **pipeline ETL** com staging e transforma√ß√£o.
* Modelagem de **banco de dados relacional**.
* Rastreabilidade e boas pr√°ticas de logging.
* Documenta√ß√£o clara e orientada para **uso real em portf√≥lio profissional**.

---

## Arquitetura (Vis√£o Geral)

```mermaid
flowchart LR
    subgraph User["üë§ Usu√°rio"]
        A1[Upload CSV com rsIDs]
    end

    subgraph Backend["‚öôÔ∏è Backend (Python/Flask)"]
        A2[Leitura do CSV]
        A3[Chamada √†s APIs NCBI e-utilities]
        A4[ETL: staging -> transforma√ß√£o -> carga]
    end

    subgraph DB["üóÑÔ∏è Banco de Dados (PostgreSQL)"]
        B1[(raw_json - staging)]
        B2[(snps)]
        B3[(variacoes)]
        B4[(genes)]
        B5[(diseases)]
        B6[(usuarios/analises)]
    end

    subgraph Frontend["üíª Frontend (Bootstrap)"]
        C1[Dashboard de Resultados]
    end

    A1 --> A2 --> A3 --> A4
    A4 --> B1 & B2 & B3 & B4 & B5 & B6
    B2 --> C1
    B3 --> C1
    B5 --> C1
```

## Diagrama de Relacionamento DB

```mermaid
erDiagram
    USUARIOS ||--o{ RAW_DATA : envia
    USUARIOS ||--o{ ANALISES : realiza

    DISEASES ||--o{ GENES : possui
    GENES ||--o{ VARIACOES : cont√©m
    VARIACOES ||--o{ VARIACAO_SNPS : est√°_relacionada
    SNPS ||--o{ VARIACAO_SNPS : est√°_relacionado
    SNPS ||--o{ RAW_DATA : aparece_em

    REFERENCIAS ||--o{ RAW_JSON : origem

    USUARIOS {
        int id_usuario
        string login
        string senha
        char sexo
        string nome_user
    }

    DISEASES {
        int id_disease
        string disease_name
        text disease_desc
    }

    GENES {
        int id_gene
        int id_disease
        string gene_symbol
        text gene_desc
    }

    VARIACOES {
        int id_variacao
        int id_gene
        string variacao
        string alelo
        text risco
    }

    SNPS {
        int id_snp
        string rsid
        string posicao_genomica
        string referencia
        text efeito
    }

    VARIACAO_SNPS {
        int id_variacao
        int id_snp
    }

    RAW_DATA {
        int id_raw_data
        int id_usuario
        int id_snp
        timestamp data_upload
    }

    ANALISES {
        int id_analise
        int id_usuario
        timestamp data
        string status
        text descricao
    }

    REFERENCIAS {
        int id_ref
        string base_origem
        text url
        timestamp data_coleta
    }

    RAW_JSON {
        int id_raw
        int id_ref
        string fonte
        jsonb payload
        timestamp data_ingestao
    }

```
---

## Como Funciona na Pr√°tica

1. O usu√°rio faz upload do seu **CSV** com identificadores de SNP (rsIDs).
2. O backend consome a API do **NCBI e-utilities** para buscar doen√ßas, genes e varia√ß√µes associadas.
3. Os dados crus s√£o armazenados em **staging (raw\_json)** para rastreabilidade.
4. Os dados transformados s√£o organizados em tabelas normalizadas (`snps`, `variacoes`, `genes`, `diseases`).
5. O frontend exibe um **resumo personalizado**, como:

   * Doen√ßas relacionadas ao usu√°rio.
   * Genes impactados.
   * SNPs de maior risco.

---
## Status do Projeto

| Indicador | Status | 
| :--- | :--- | 
| **Desenvolvimento** | Funcionalidades principais conclu√≠das | 
| **Deploy (Infra)** | **CONCLU√çDO** | 
| **Ambiente** | Produ√ß√£o (AWS) | 
| **Seguran√ßa** | HTTPS (Certificado ACM V√°lido) | 
| **Acesso** | `https://dnaengine.com.br` |
---

## Arquitetura de Produ√ß√£o (AWS)

**Infraestrutura de produ√ß√£o garante alta disponibilidade, escalabilidade e seguran√ßa de ponta a ponta.**

| Componente | Fun√ß√£o | Tecnologia | 
| :--- | :--- | :--- | 
| **Containers** | Servi√ßo de aplica√ß√£o principal | AWS ECS / Fargate | 
| **Banco de Dados** | Persist√™ncia e gerenciamento de dados | AWS RDS (PostgreSQL) | 
| **Roteamento & Balanceamento** | Distribui√ß√£o de tr√°fego, redirecionamento HTTP->HTTPS | AWS Application Load Balancer (ALB) | 
| **Dom√≠nio & DNS** | Gerenciamento de registros e delega√ß√£o | AWS Route 53 | 
| **Seguran√ßa** | Emiss√£o e gerenciamento de certificado SSL/TLS | AWS Certificate Manager (ACM) |
---

1. **Implementa√ß√£o de Conteineriza√ß√£o:** Finaliza√ß√£o da imagem Docker e configura√ß√£o dos servi√ßos para execu√ß√£o no ambiente de produ√ß√£o.

2. **Deploy Cont√≠nuo na AWS (Fargate/ECS):** Configura√ß√£o de um Cluster ECS utilizando **AWS Fargate** (Container as a Service), eliminando a necessidade de gerenciamento de servidores e garantindo escalabilidade autom√°tica.

3. **Infraestrutura de Banco de Dados Gerenciada:** Migra√ß√£o e configura√ß√£o do banco de dados para **Amazon RDS (PostgreSQL)**, garantindo backups autom√°ticos, alta resili√™ncia e menor sobrecarga operacional.

4. **Configura√ß√£o Final de Roteamento e Seguran√ßa (HTTPS):**

   * **Gerenciamento DNS:** Migra√ß√£o da delega√ß√£o do dom√≠nio para o **Amazon Route 53** para integra√ß√£o nativa com servi√ßos AWS.

   * **Certifica√ß√£o SSL/TLS:** Obten√ß√£o e valida√ß√£o do certificado SSL/TLS (ACM) no dom√≠nio principal.

   * **Publica√ß√£o Segura:** Configura√ß√£o do **Application Load Balancer (ALB)** com Listener HTTPS na porta 443, for√ßando o tr√°fego seguro e garantindo o sucesso da publica√ß√£o do dom√≠nio com HTTPS.

   * **Resolu√ß√£o de Problemas:** Conclus√£o do processo de valida√ß√£o, que exigiu a **espera pela propaga√ß√£o global do DNS** ap√≥s a delega√ß√£o de servidores.

5. **Gerenciamento de Acesso:** Configura√ß√£o de Security Groups (SGs) restritivos para comunica√ß√£o segura entre ALB, Fargate e RDS, incluindo acesso controlado via PgAdmin.

---
## Melhorias Futuras (N√≠vel Pleno ‚Üí Avan√ßado)

* Adicionar **Airflow/Prefect** para orquestra√ß√£o do pipeline.
* Exportar dados em **Parquet/JSON** simulando Data Lake.
* Implementar **testes unit√°rios** para parsing das APIs.
* Containeriza√ß√£o com **Docker** (Postgres + Backend).
* Criar caching de consultas √†s APIs do NCBI.
* Dashboard avan√ßado (Gr√°ficos + estat√≠sticas por gene/doen√ßa).

---

Este reposit√≥rio ser√° atualizado √† medida que cada tarefa for conclu√≠da.
Objetivo final: um projeto completo de **engenharia de dados aplicada √† sa√∫de**.


---


## Obst√°culos T√©cnicos e Desafios Enfrentados

Durante o desenvolvimento do projeto **Genome Data Pipeline**, diversos desafios t√©cnicos surgiram, exigindo adapta√ß√µes criativas e decis√µes estrat√©gicas. Abaixo est√£o os principais obst√°culos enfrentados:

- **Limita√ß√µes de APIs externas**  
  As APIs do NCBI (e-utilities) possuem restri√ß√µes de taxa e estrutura de resposta complexa. Foi necess√°rio implementar controle de tempo entre requisi√ß√µes e tratamento robusto de erros para garantir a estabilidade do pipeline.

- **Tradu√ß√£o de dados biom√©dicos**  
  A tradu√ß√£o autom√°tica das descri√ß√µes de doen√ßas (em ingl√™s) para o portugu√™s apresentou dificuldades. Bibliotecas como `googletrans` deixaram de funcionar em vers√µes recentes do Python (como 3.13), e APIs como DeepL e Google Cloud Translate exigem planos pagos. A solu√ß√£o adotada foi realizar a tradu√ß√£o manual dos textos mais relevantes.

- **Limita√ß√µes de bibliotecas em Python 3.13**  
  Algumas bibliotecas amplamente utilizadas, como `googletrans`, ainda n√£o s√£o compat√≠veis com Python 3.13 devido √† remo√ß√£o de m√≥dulos como `cgi`. Isso exigiu reavalia√ß√£o de depend√™ncias e busca por alternativas compat√≠veis.

- **Documenta√ß√£o e visualiza√ß√£o**  
  Traduzir a complexidade t√©cnica do pipeline para uma documenta√ß√£o clara e acess√≠vel foi um desafio √† parte. O objetivo foi tornar o projeto compreens√≠vel tanto para profissionais t√©cnicos quanto para recrutadores e colegas de √°rea.
